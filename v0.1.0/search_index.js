var documenterSearchIndex = {"docs":
[{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"CurrentModule = FlashRank","category":"page"},{"location":"api_reference/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"API reference for FlashRank.","category":"page"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"Modules = [FlashRank]","category":"page"},{"location":"api_reference/#FlashRank.BertTextEncoder","page":"API Reference","title":"FlashRank.BertTextEncoder","text":"BertTextEncoder\n\nThe text encoder for Bert model (WordPiece tokenization).\n\nFields\n\nwp::WordPiece: The WordPiece tokenizer.\nvocab::Dict{String, Int}: The vocabulary, 0-based indexing of tokens to match Python implementation.\nstartsym::String: The start symbol.\nendsym::String: The end symbol.\npadsym::String: The pad symbol.\ntrunc::Union{Nothing, Int}: The truncation length. Defaults to 512 tokens.\n\n\n\n\n\n","category":"type"},{"location":"api_reference/#FlashRank.RankerModel","page":"API Reference","title":"FlashRank.RankerModel","text":"RankerModel\n\nA model for ranking passages, including the encoder and the ONNX session for inference.\n\nFor ranking, use as rank(ranker, query, passages) or as a functor ranker(query, passages).\n\n\n\n\n\n","category":"type"},{"location":"api_reference/#FlashRank.WordPiece","page":"API Reference","title":"FlashRank.WordPiece","text":"WordPiece\n\nWordPiece is a tokenizer that splits a string into a sequence of KNOWN sub-word tokens (or token IDs). It uses a double array trie to store the vocabulary and the index of the vocabulary.\n\nImplementation is based on: https://github.com/chengchingwen/Transformers.jl\n\nFields\n\ntrie::DoubleArrayTrie: The double array trie of the vocabulary (for fast lookups of tokens).\nindex::Vector{Int}: The index of the vocabulary. It is 0-based as we provide token IDs to models trained in python.\nunki::Int: The index of the unknown token in the TRIE (ie, this is not the token ID, but the trie index).\nmax_char::Int: The maximum number of characters in a token. Default is 200.\nsubword_prefix::String: The prefix of a sub-word token. Default is \"##\".\n\n\n\n\n\n","category":"type"},{"location":"api_reference/#FlashRank.WordPiece-Tuple{AbstractString}","page":"API Reference","title":"FlashRank.WordPiece","text":"(wp::WordPiece; token_ids::Bool = false)(x)\n\nWordPiece functor that tokenizes a string into a sequence of tokens (or token IDs).\n\nArguments\n\ntoken_ids::Bool = false: If true, return the token IDs directly. Otherwise, return the tokens.\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#FlashRank.bert_cased_tokenizer-Tuple{Any}","page":"API Reference","title":"FlashRank.bert_cased_tokenizer","text":"bert_cased_tokenizer(input)\n\nGoogle bert tokenizer which remain the case during tokenization. Recommended for multi-lingual data.\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#FlashRank.bert_uncased_tokenizer-Tuple{Any}","page":"API Reference","title":"FlashRank.bert_uncased_tokenizer","text":"bert_uncased_tokenizer(input)\n\nGoogle bert tokenizer which do lower case on input before tokenization.\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#FlashRank.rank-Tuple{RankerModel, AbstractString, AbstractVector{<:AbstractString}}","page":"API Reference","title":"FlashRank.rank","text":"rank(\n    ranker::RankerModel, query::AbstractString, passages::AbstractVector{<:AbstractString};\n    top_n = length(passages))\n\nRanks passages for a given query using the given ranker model. Ranking should determine higher suitability to provide an answer to the query (higher score is better).\n\nArguments:\n\nranker::RankerModel: The ranker model to use.\nquery::AbstractString: The query to rank passages for.\npassages::AbstractVector{<:AbstractString}: The passages to rank.\ntop_n: The number of most relevant documents to return. Default is length(passages).\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#FlashRank.tokenize-Tuple{FlashRank.BertTextEncoder, AbstractString}","page":"API Reference","title":"FlashRank.tokenize","text":"tokenize(enc::BertTextEncoder, text::AbstractString;\n    add_special_tokens::Bool = true, add_end_token::Bool = true, token_ids::Bool = false)\n\nTokenizes the text and returns the tokens or token IDs (to skip looking up the IDs twice).\n\nArguments\n\nadd_special_tokens::Bool = true: Add special tokens at the beginning and end of the text.\nadd_end_token::Bool = true: Add end token at the end of the text.\ntoken_ids::Bool = false: If true, return the token IDs directly. Otherwise, return the tokens.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = FlashRank","category":"page"},{"location":"#FlashRank.jl","page":"Home","title":"FlashRank.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FlashRank.jl is inspired by the awesome FlashRank Python package, originally developed by Prithiviraj Damodaran. This package leverages model weights from Prithiviraj's repository on Hugging Face and provides a fast and efficient way to rank documents relevant to any given query without GPUs and large dependencies. This enhances Retrieval Augmented Generation (RAG) pipelines by prioritizing the most suitable documents. The smallest model can be run on almost any machine.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Two ranking models:\nTiny (~4MB): ms-marco-TinyBERT-L-2-v2 (default) (alias :tiny)\nMini (~23MB): ms-marco-MiniLM-L-12-v2 (alias :mini)\nLightweight dependencies, avoiding heavy frameworks like Flux and CUDA for ease of integration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"How fast is it?  With the Tiny model, you can rank 100 documents in ~0.1 seconds on a laptop. With the Mini model, you can rank 20 documents in ~0.5 seconds to pick the best chunks for your context.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that we're using BERT models with a maximum chunk size of 512 tokens (anything over will be truncated).","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install FlashRank.jl, simply add this repository (package is not yet registered).","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"https://github.com/svilupp/FlashRank.jl\")","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Ranking your documents for a given query is as simple as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FlashRank\n\nranker = RankerModel() # Defaults to model = `:tiny`\n\nquery = \"How to speedup LLMs?\"\npassages = [\n        \"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.\",\n        \"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more \\$\\$\\$ you will save. vllm project is a must-read for this direction, and now they have just released the paper\",\n        \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods Iâ€™ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n        \"Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.\",\n        \"vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels\",\n];\n\n\nresult = rank(ranker, query, passages)","category":"page"},{"location":"","page":"Home","title":"Home","text":"result is of type RankResult and contains the sorted passages, their scores (0-1, where 1 is the best) and the positions of the sorted documents (referring to the original passages vector).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here's a brief outline of how you can integrate FlashRank.jl into your PromptingTools.jl RAG pipeline.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a full example, see examples/prompting_tools_integration.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FlashRank\nusing PromptingTools\nusing PromptingTools.Experimental.RAGTools\nconst RT = PromptingTools.Experimental.RAGTools\n\n# Wrap the model to be a valid Ranker recognized by RAGTools\n# It will be provided to the airag/rerank function to avoid instantiating it on every call\nstruct FlashRanker <: RT.AbstractReranker\n    model::RankerModel\nend\nreranker = RankerModel(:tiny) |> FlashRanker\n\n# Define the method for ranking with it\nfunction RT.rerank(\n        reranker::FlashRanker, index::RT.AbstractDocumentIndex, question::AbstractString,\n        candidates::RT.AbstractCandidateChunks; kwargs...)\n    ## omitted for brevity\n    ## See examples/prompting_tools_integration.jl for details\nend\n\n## Apply to the pipeline configuration, eg, \ncfg = RAGConfig(; retriever=RT.AdvancedRetriever(; reranker))\n## assumes existing index\nquestion = \"Tell me about prehistoric animals\"\nresult = airag(cfg, index; question, return_all = true)","category":"page"},{"location":"#Acknowledgments","page":"Home","title":"Acknowledgments","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FlashRank and Transformers.jl have been essential in the development of this package.\nSpecial thanks to Prithiviraj Damodaran for the original FlashRank and model weights.\nAnd to Transformers.jl for the WordPiece implementation and BERT tokenizer which have been forked for this package (to minimize dependencies).","category":"page"}]
}
